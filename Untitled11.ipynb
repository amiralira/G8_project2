{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import gradio\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "# import gradio\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "# from transformers import BertTokenizer\n",
        "import re\n",
        "# import unicodedata\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import nltk\n",
        "# from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "NgXluIY1-Q7B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/sales_data.csv'"
      ],
      "metadata": {
        "id": "_Upq3B-5-0lK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(root)\n"
      ],
      "metadata": {
        "id": "9ykD3dXv-KN6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Branch, Manufacturer, Model_Name, Category, Screen_Size, Screen, CPU\n",
        "#  10  RAM\n",
        "#  11  Storage\n",
        "# #  12  GPU     Weight\n",
        "#  16  Price"
      ],
      "metadata": {
        "id": "I3oQwinT-2x3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkeHR5n3_oSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw88t-cc_EtA",
        "outputId": "f9b2601d-eee3-4647-efeb-88beb71640e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 295600 entries, 0 to 295599\n",
            "Data columns (total 22 columns):\n",
            " #   Column          Non-Null Count   Dtype  \n",
            "---  ------          --------------   -----  \n",
            " 0   Order_ID        295600 non-null  int64  \n",
            " 1   Branch          295600 non-null  object \n",
            " 2   Order_Date      295600 non-null  object \n",
            " 3   Order_Priority  295600 non-null  object \n",
            " 4   Manufacturer    295600 non-null  object \n",
            " 5   Model_Name      295600 non-null  object \n",
            " 6   Category        295600 non-null  object \n",
            " 7   Screen_Size     295600 non-null  object \n",
            " 8   Screen          295600 non-null  object \n",
            " 9   CPU             295600 non-null  object \n",
            " 10  RAM             295600 non-null  object \n",
            " 11  Storage         295600 non-null  object \n",
            " 12  GPU             295600 non-null  object \n",
            " 13  OS              295600 non-null  object \n",
            " 14  OS_Version      264196 non-null  object \n",
            " 15  Weight          295600 non-null  object \n",
            " 16  Price           295600 non-null  int64  \n",
            " 17  Quantity        295600 non-null  int64  \n",
            " 18  Discount        295600 non-null  int64  \n",
            " 19  Total_Price     295600 non-null  float64\n",
            " 20  Profit          295600 non-null  int64  \n",
            " 21  Ship_Duration   295599 non-null  float64\n",
            "dtypes: float64(2), int64(5), object(15)\n",
            "memory usage: 49.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smM-Ikgb-Unr"
      },
      "outputs": [],
      "source": [
        "\n",
        "from App.tfidfrecommender import TfidfRecommender\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "desc = pd.read_csv(root)\n",
        "\n",
        "rec = TfidfRecommender(desc, 'id', 'description' , \"none\")\n",
        "def initialize_and_tokenize(tokenizer):\n",
        "    print(\"tok\")\n",
        "    rec.tokenization_method = tokenizer\n",
        "    rec.tokenize_text()\n",
        "\n",
        "names = []\n",
        "def recommend (movies, tok) :\n",
        "    rec.tokenization_method = tok\n",
        "    tf, vecs = rec.tokenize_text()\n",
        "    rec.fit(tf, vecs)\n",
        "    print(\"rec\")\n",
        "    pool = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
        "    futures = [pool.submit(rec.recommend_k_items, movie, 5) for movie in movies]\n",
        "    idss = []\n",
        "    print(\"after submit\")\n",
        "    for i in range(len(futures)):\n",
        "        print(\"res\")\n",
        "        idss.append(futures[i].result())\n",
        "    print(\"shutdown\")\n",
        "    pool.shutdown(wait=True)\n",
        "    ids = [id for ids in idss for id in ids]\n",
        "    ids = list(set(ids))\n",
        "    names = desc[desc['id'].isin(ids)]['title'].to_list()\n",
        "    return ', '.join(names)\n",
        "\n",
        "def recom(movies, tok):\n",
        "    rec.tokenization_method = tok\n",
        "    tf, vecs = rec.tokenize_text()\n",
        "    rec.fit(tf, vecs)\n",
        "    print(movies[0])\n",
        "    ids = rec.recommend_k_items(movies[0], 5)\n",
        "    print(\"reccc\")\n",
        "    # ids = list(set(ids))\n",
        "    names = desc[desc['id'].isin(ids)]['title'].to_list()\n",
        "    return ', '.join(names)\n",
        "\n",
        "demo = gr.Interface(fn=recom,\n",
        "             inputs=[gr.Dropdown(choices = list(desc['title'][:20]), multiselect=True, max_choices=3, label=\"Movies\"),\n",
        "                     gr.Radio([\"bert\", \"scibert\", \"nltk\" , \"none\"], value=\"none\", label=\"Tokenization and text preprocess\")],\n",
        "             outputs=gr.Textbox(label=\"Recommended\"))\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck5y1Y3J9Vf7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import gradio as gr\n",
        "root = '/content/sales_data.csv'\n",
        "desc = pd.read_csv(root)\n",
        "\n",
        "rec = TfidfRecommender(desc, 'id', 'description' , \"none\")\n",
        "def initialize_and_tokenize(tokenizer):\n",
        "    print(\"tok\")\n",
        "    rec.tokenization_method = tokenizer\n",
        "    rec.tokenize_text()\n",
        "\n",
        "names = []\n",
        "def recommend (movies, tok) :\n",
        "    rec.tokenization_method = tok\n",
        "    tf, vecs = rec.tokenize_text()\n",
        "    rec.fit(tf, vecs)\n",
        "    print(\"rec\")\n",
        "    pool = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
        "    futures = [pool.submit(rec.recommend_k_items, movie, 5) for movie in movies]\n",
        "    idss = []\n",
        "    print(\"after submit\")\n",
        "    for i in range(len(futures)):\n",
        "        print(\"res\")\n",
        "        idss.append(futures[i].result())\n",
        "    print(\"shutdown\")\n",
        "    pool.shutdown(wait=True)\n",
        "    ids = [id for ids in idss for id in ids]\n",
        "    ids = list(set(ids))\n",
        "    names = desc[desc['id'].isin(ids)]['title'].to_list()\n",
        "    return ', '.join(names)\n",
        "\n",
        "def recom(movies, tok):\n",
        "    rec.tokenization_method = tok\n",
        "    tf, vecs = rec.tokenize_text()\n",
        "    rec.fit(tf, vecs)\n",
        "    print(movies[0])\n",
        "    ids = rec.recommend_k_items(movies[0], 5)\n",
        "    print(\"reccc\")\n",
        "    # ids = list(set(ids))\n",
        "    names = desc[desc['id'].isin(ids)]['title'].to_list()\n",
        "    return ', '.join(names)\n",
        "\n",
        "demo = gr.Interface(fn=recom,\n",
        "             inputs=[gr.Dropdown(choices = list(desc['title'][:20]), multiselect=True, max_choices=3, label=\"Movies\"),\n",
        "                     gr.Radio([\"bert\", \"scibert\", \"nltk\" , \"none\"], value=\"none\", label=\"Tokenization and text preprocess\")],\n",
        "             outputs=gr.Textbox(label=\"Recommended\"))\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TfidfRecommender :\n",
        "    def __init__(self, df, id_col, text_col, tokenization_method) :\n",
        "        \"\"\"Initialize model parameters\n",
        "        Args:\n",
        "            id_col (str): Name of column containing item IDs.\n",
        "            tokenization_method (str): ['none','nltk','bert','scibert'] option for tokenization method.\n",
        "        \"\"\"\n",
        "        self.id_col = id_col\n",
        "        self.text_col = text_col\n",
        "        self.df = df\n",
        "\n",
        "        if tokenization_method.lower() not in [\"none\", \"nltk\", \"bert\", \"scibert\"]:\n",
        "            raise ValueError(\n",
        "                'Tokenization method must be one of [\"none\" | \"nltk\" | \"bert\" | \"scibert\"]'\n",
        "            )\n",
        "        self.tokenization_method = tokenization_method.lower()\n",
        "\n",
        "        # Initialize other variables used in this class\n",
        "        self.tf = TfidfVectorizer()\n",
        "        self.tfidf_matrix = dict()\n",
        "        self.tokens = dict()\n",
        "        self.stop_words = frozenset()\n",
        "        self.recommendations = dict()\n",
        "        self.top_k_recommendations = pd.DataFrame()\n",
        "\n",
        "    def __clean_text (self, text, for_Bert=False, verbose=False) :\n",
        "        try:\n",
        "            # Remove new line and tabs\n",
        "            clean = text.replace(\"\\n\", \" \")\n",
        "            clean = clean.replace(\"\\t\", \" \")\n",
        "            clean = clean.replace(\"\\r\", \" \")\n",
        "            clean = clean.replace(\"Ã‚\\xa0\", \"\")  # non-breaking space\n",
        "\n",
        "            # Remove all punctuation and special characters\n",
        "            # clean = re.sub(\n",
        "            #     r\"([^\\s\\w]|_)+\", \"\", clean\n",
        "            # )  # noqa W695 invalid escape sequence '\\s'\n",
        "\n",
        "            # If you want to keep some punctuation, see below commented out example\n",
        "            clean = re.sub(r'([^,.:\\s\\w\\-]|_)+','', clean)\n",
        "\n",
        "            # Skip further processing if the text will be used in BERT tokenization\n",
        "            if for_Bert is False:\n",
        "                # Lower case\n",
        "                clean = clean.lower()\n",
        "                clean = re.sub(\n",
        "                r\"([^\\s\\w]|_)+\", \"\", clean\n",
        "            )\n",
        "        except Exception:\n",
        "            if verbose :\n",
        "                print(\"Cannot clean non-existent text\")\n",
        "            clean = \"\"\n",
        "\n",
        "        return clean\n",
        "\n",
        "    def _clean_df (self):\n",
        "        self.df = self.df.replace(np.nan, \"\", regex=True)\n",
        "        # df[new_col_name] = df[cols_to_clean].apply(lambda cols: \" \".join(cols), axis=1)\n",
        "\n",
        "        # Check if for BERT tokenization\n",
        "        if self.tokenization_method in [\"bert\", \"scibert\"]:\n",
        "            for_BERT = True\n",
        "        else:\n",
        "            for_BERT = False\n",
        "\n",
        "        # Clean the text in the dataframe\n",
        "        self.df[self.text_col] = self.df[self.text_col].map(\n",
        "            lambda x: self.__clean_text(x, for_BERT)\n",
        "        )\n",
        "\n",
        "    def tokenize_text (self, ngram_range=(1, 3), min_df=0.0) :\n",
        "        \"\"\"Tokenize the input text.\n",
        "        Args:\n",
        "            df_clean (pandas.DataFrame): Dataframe with cleaned text in the new column.\n",
        "            text_col (str): Name of column containing the cleaned text.\n",
        "            ngram_range (tuple of int): The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
        "            min_df (int): When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "        Returns:\n",
        "            TfidfVectorizer, pandas.Series:\n",
        "            - Scikit-learn TfidfVectorizer object defined in `.tokenize_text()`.\n",
        "            - Each row contains tokens for respective documents separated by spaces.\n",
        "        \"\"\"\n",
        "        self._clean_df()\n",
        "        vectors = self.df[self.text_col]\n",
        "\n",
        "        if self.tokenization_method in [\"bert\", \"scibert\"] :\n",
        "            # vectorizer\n",
        "            tf = TfidfVectorizer(\n",
        "                analyzer=\"word\",\n",
        "                ngram_range=ngram_range,\n",
        "                min_df=min_df,\n",
        "                stop_words=\"english\",\n",
        "            )\n",
        "\n",
        "            if self.tokenization_method == \"bert\":\n",
        "                bert_method = \"bert-base-cased\"\n",
        "            elif self.tokenization_method == \"scibert\":\n",
        "                bert_method = \"allenai/scibert_scivocab_cased\"\n",
        "\n",
        "            # Load pre-trained bert model (vocabulary)\n",
        "            tokenizer = BertTokenizer.from_pretrained(bert_method)\n",
        "\n",
        "            # tokenization\n",
        "            vectors_tokenized = vectors.copy()\n",
        "            for i in range(0, len(vectors)):\n",
        "                vectors_tokenized[i] = \" \".join(tokenizer.tokenize(vectors[i]))\n",
        "\n",
        "        elif self.tokenization_method == \"nltk\":\n",
        "            # NLTK Stemming\n",
        "            token_dict = {}  # noqa: F841\n",
        "            stemmer = PorterStemmer()\n",
        "\n",
        "            def stem_tokens(tokens, stemmer):\n",
        "                stemmed = []\n",
        "                for item in tokens:\n",
        "                    stemmed.append(stemmer.stem(item))\n",
        "                return stemmed\n",
        "\n",
        "            def tokenize(text):\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                stems = stem_tokens(tokens, stemmer)\n",
        "                return stems\n",
        "\n",
        "            # The tokenization using a custom tokenizer is applied in the fit function\n",
        "            tf = TfidfVectorizer(\n",
        "                tokenizer=tokenize,\n",
        "                analyzer=\"word\",\n",
        "                ngram_range=ngram_range,\n",
        "                min_df=min_df,\n",
        "                stop_words=\"english\",\n",
        "            )\n",
        "            vectors_tokenized = vectors\n",
        "\n",
        "        elif self.tokenization_method == \"none\":\n",
        "            # No tokenization applied\n",
        "            tf = TfidfVectorizer(\n",
        "                analyzer=\"word\",\n",
        "                ngram_range=ngram_range,\n",
        "                min_df=min_df,\n",
        "                stop_words=\"english\",\n",
        "            )\n",
        "            vectors_tokenized = vectors\n",
        "\n",
        "        # Save to class variable\n",
        "        self.tf = tf\n",
        "\n",
        "        return tf, vectors_tokenized\n",
        "\n",
        "\n",
        "    def fit (self, tf, vectors_tokenized) :\n",
        "        self.tfidf_matrix = tf.fit_transform(vectors_tokenized)\n",
        "\n",
        "    def get_tokens (self) :\n",
        "        try:\n",
        "            self.tokens = self.tf.vocabulary_\n",
        "        except Exception:\n",
        "            self.tokens = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
        "        return self.tokens\n",
        "\n",
        "    def get_stop_words (self) :\n",
        "        try:\n",
        "            self.stop_words = self.tf.get_stop_words()\n",
        "        except Exception:\n",
        "            self.stop_words = \"Run .tokenize_text() and .fit_tfidf() first\"\n",
        "        return self.stop_words\n",
        "\n",
        "    def recommend_k_items (self, title, k) :\n",
        "        print(\"jjj\")\n",
        "        idx = self.df[self.df['title'] == title].index[0]\n",
        "        print(\"ppp\")\n",
        "        cosine_sim = cosine_similarity(self.tfidf_matrix[int(idx)], self.tfidf_matrix)\n",
        "        similarity_scores = list(enumerate(cosine_sim[0]))\n",
        "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "        similarity_scores = similarity_scores[1: k + 1]\n",
        "        print(\"lol\")\n",
        "        movie_indices = [i[0] for i in similarity_scores]\n",
        "        return self.df.iloc[movie_indices]['id']"
      ],
      "metadata": {
        "id": "kuxRG4SC9q2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgCuwuw0-ORr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}